{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from madminer.ml.ensemble import Ensemble\n",
    "from madminer.ml import ParameterizedRatioEstimator, ScoreEstimator, BayesianScoreEstimator, HeteroskedasticScoreEstimator, RepulsiveEnsembleScoreEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data2\"):\n",
    "    os.makedirs(\"data2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s\",\n",
    "    datefmt=\"%H:%M\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_std = 2.0\n",
    "x_std = 1.0\n",
    "\n",
    "def simulate(theta, theta0=None, theta1=None, theta_score=None, npoints=None):\n",
    "    # Draw latent variables z\n",
    "    z = np.random.normal(loc = theta[0] + theta[1], scale=z_std, size=npoints)\n",
    "\n",
    "    # Draw observables x\n",
    "    x = np.random.normal(loc=z, scale=x_std, size=None)\n",
    "\n",
    "    t_xz = np.array([2*(z - theta[0] - theta[1]) / z_std**2, 2*(z - theta[0] - theta[1]) / z_std**2]).T\n",
    "\n",
    "    return x, t_xz\n",
    "\n",
    "def calculate_true_score(x, theta):\n",
    "    combined_std = (z_std**2 + x_std**2) ** 0.5\n",
    "    t_x = np.array([2*(x - theta[0] - theta[1])/combined_std**2, 2*(x - theta[0] - theta[1])/combined_std**2])\n",
    "    return t_x\n",
    "\n",
    "def calculate_true_info(x, theta):\n",
    "    score = calculate_true_score(x, theta)\n",
    "    return np.einsum('jn,kn->njk', score, score)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parameter points to train\n",
    "n_param_points = 1000\n",
    "\n",
    "# numerator, uniform prior\n",
    "theta0 = np.zeros(shape=(2,n_param_points))\n",
    "\n",
    "# Sample from theta0\n",
    "x_train, t_xz_train = simulate(theta0, theta0, theta0, theta0)\n",
    "\n",
    "# Save to file\n",
    "np.save(\"data2/x_train.npy\", x_train)\n",
    "np.save(\"data2/t_xz_train.npy\", t_xz_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:42 madminer.ml.score    INFO    Starting training\n",
      "16:42 madminer.ml.score    INFO      Batch size:             128\n",
      "16:42 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "16:42 madminer.ml.score    INFO      Epochs:                 1000\n",
      "16:42 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:42 madminer.ml.score    INFO      Validation split:       0.25\n",
      "16:42 madminer.ml.score    INFO      Early stopping:         True\n",
      "16:42 madminer.ml.score    INFO      Scale inputs:           True\n",
      "16:42 madminer.ml.score    INFO      Shuffle labels          False\n",
      "16:42 madminer.ml.score    INFO      Samples:                all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:42 madminer.ml.score    INFO    Loading training data\n",
      "16:42 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "16:42 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "16:42 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "16:42 madminer.ml.base     INFO    Setting up input rescaling\n",
      "16:42 madminer.ml.score    INFO    Creating model\n",
      "16:42 madminer.ml.score    INFO    Training model\n",
      "16:42 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "16:42 madminer.utils.ml.tr INFO      Epoch  50: train loss -44.72663 (repulsive_ensemble_loss: -44.727)\n",
      "16:42 madminer.utils.ml.tr INFO                 val. loss  -65.07101 (repulsive_ensemble_loss: -65.071)\n",
      "16:42 madminer.utils.ml.tr INFO      Epoch 100: train loss -44.88999 (repulsive_ensemble_loss: -44.890)\n",
      "16:42 madminer.utils.ml.tr INFO                 val. loss  -64.58955 (repulsive_ensemble_loss: -64.590)\n",
      "16:42 madminer.utils.ml.tr INFO      Epoch 150: train loss -44.74053 (repulsive_ensemble_loss: -44.741)\n",
      "16:42 madminer.utils.ml.tr INFO                 val. loss  -63.89869 (repulsive_ensemble_loss: -63.899)\n",
      "16:42 madminer.utils.ml.tr INFO      Epoch 200: train loss -45.55818 (repulsive_ensemble_loss: -45.558)\n",
      "16:42 madminer.utils.ml.tr INFO                 val. loss  -63.69555 (repulsive_ensemble_loss: -63.696)\n",
      "16:42 madminer.utils.ml.tr INFO      Epoch 250: train loss -46.12441 (repulsive_ensemble_loss: -46.124)\n",
      "16:42 madminer.utils.ml.tr INFO                 val. loss  -62.57503 (repulsive_ensemble_loss: -62.575)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 300: train loss -45.79632 (repulsive_ensemble_loss: -45.796)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.87500 (repulsive_ensemble_loss: -62.875)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 350: train loss -46.14794 (repulsive_ensemble_loss: -46.148)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.66908 (repulsive_ensemble_loss: -62.669)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 400: train loss -46.02886 (repulsive_ensemble_loss: -46.029)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -63.16137 (repulsive_ensemble_loss: -63.161)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 450: train loss -45.41013 (repulsive_ensemble_loss: -45.410)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -63.29790 (repulsive_ensemble_loss: -63.298)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 500: train loss -45.81426 (repulsive_ensemble_loss: -45.814)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -63.04296 (repulsive_ensemble_loss: -63.043)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 550: train loss -46.17518 (repulsive_ensemble_loss: -46.175)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -63.18015 (repulsive_ensemble_loss: -63.180)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 600: train loss -45.85647 (repulsive_ensemble_loss: -45.856)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.83809 (repulsive_ensemble_loss: -62.838)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 650: train loss -45.64178 (repulsive_ensemble_loss: -45.642)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.62044 (repulsive_ensemble_loss: -62.620)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 700: train loss -46.01086 (repulsive_ensemble_loss: -46.011)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.76925 (repulsive_ensemble_loss: -62.769)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 750: train loss -46.17888 (repulsive_ensemble_loss: -46.179)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.97377 (repulsive_ensemble_loss: -62.974)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 800: train loss -46.14465 (repulsive_ensemble_loss: -46.145)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.71963 (repulsive_ensemble_loss: -62.720)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 850: train loss -46.63030 (repulsive_ensemble_loss: -46.630)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.84152 (repulsive_ensemble_loss: -62.842)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 900: train loss -46.33308 (repulsive_ensemble_loss: -46.333)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.60327 (repulsive_ensemble_loss: -62.603)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 950: train loss -46.49705 (repulsive_ensemble_loss: -46.497)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.93130 (repulsive_ensemble_loss: -62.931)\n",
      "16:43 madminer.utils.ml.tr INFO      Epoch 1000: train loss -46.20193 (repulsive_ensemble_loss: -46.202)\n",
      "16:43 madminer.utils.ml.tr INFO                 val. loss  -62.83944 (repulsive_ensemble_loss: -62.839)\n",
      "16:43 madminer.utils.ml.tr INFO    Early stopping after epoch 46, with loss -65.12792 compared to final loss -62.83944\n",
      "16:43 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "16:43 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                                   ALL:   0.02h\n",
      "16:43 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.01h\n",
      "16:43 madminer.utils.ml.tr INFO                 training forward pass:   0.01h\n",
      "16:43 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                         opt: backward:   0.01h\n",
      "16:43 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                        optimizer step:   0.01h\n",
      "16:43 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "16:43 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "16:43 madminer.ml.base     INFO    Saving model to models2/repulsive_ensemble_sally\n"
     ]
    }
   ],
   "source": [
    "# import cProfile\n",
    "\n",
    "repulsive_ensemble_sally = RepulsiveEnsembleScoreEstimator(n_hidden=(20, 20), activation=\"relu\")\n",
    "\n",
    "# cProfile.run(\"\"\"\n",
    "repulsive_ensemble_sally.train(\n",
    "    method=\"repulsive_ensemble_sally\",\n",
    "    x=\"data2/x_train.npy\",\n",
    "    t_xz=\"data2/t_xz_train.npy\",\n",
    "    n_epochs=1000,\n",
    "    initial_lr=0.001,\n",
    "    optimizer_kwargs={\"weight_decay\": 1.0 / (2 * n_param_points * 1.0**2)},\n",
    "    scheduler=True\n",
    ")\n",
    "# \"\"\", \"trainstats\")\n",
    "repulsive_ensemble_sally.save(\"models2/repulsive_ensemble_sally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 1000\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17657 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19253 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17434 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19316 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 150: train loss  0.17387 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19294 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 200: train loss  0.17331 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19297 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 250: train loss  0.17396 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19296 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 300: train loss  0.17363 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19372 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 350: train loss  0.17365 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19414 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 400: train loss  0.17351 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19415 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 450: train loss  0.17241 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19480 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 500: train loss  0.17257 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19355 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 550: train loss  0.17295 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19478 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 600: train loss  0.17269 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19356 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 650: train loss  0.17287 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19368 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 700: train loss  0.17362 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19367 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 750: train loss  0.17269 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19380 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 800: train loss  0.17276 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19490 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 850: train loss  0.17315 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19386 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 900: train loss  0.17260 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19485 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 950: train loss  0.17316 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19456 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 1000: train loss  0.17325 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19430 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 48, with loss  0.19156 compared to final loss  0.19430\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally\n"
     ]
    }
   ],
   "source": [
    "sally = ScoreEstimator(n_hidden=(20, 20), activation=\"relu\")\n",
    "\n",
    "sally.train(\n",
    "    method=\"sally\",\n",
    "    x=\"data2/x_train.npy\",\n",
    "    t_xz=\"data2/t_xz_train.npy\",\n",
    "    n_epochs=1000\n",
    ")\n",
    "\n",
    "sally.save(\"models2/sally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.67134 (mse_score:  0.671)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.78738 (mse_score:  0.787)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.43192 (mse_score:  0.432)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.51117 (mse_score:  0.511)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26404 (mse_score:  0.264)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30898 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19746 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22660 (mse_score:  0.227)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18767 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20557 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18393 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20133 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18143 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19833 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18027 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19692 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17919 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19465 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17885 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19350 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17911 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19285 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17870 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19186 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17834 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19247 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17765 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17789 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19088 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17746 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19160 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17750 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19113 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17779 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19033 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17855 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19028 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17715 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18990 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 94, with loss  0.18973 compared to final loss  0.18990\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_0\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.81835 (mse_score:  0.818)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.74058 (mse_score:  0.741)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.60861 (mse_score:  0.609)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54666 (mse_score:  0.547)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.41064 (mse_score:  0.411)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37286 (mse_score:  0.373)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.28180 (mse_score:  0.282)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26462 (mse_score:  0.265)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22304 (mse_score:  0.223)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21556 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20149 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20005 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19613 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19392 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19114 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19035 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18972 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18911 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18714 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18892 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18712 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18694 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18483 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18616 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18429 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18622 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18474 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18463 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18411 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18414 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18313 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18433 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18256 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18410 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18228 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18151 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18378 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18226 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18406 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 94, with loss  0.18341 compared to final loss  0.18406\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_1\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.59460 (mse_score:  0.595)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.51315 (mse_score:  0.513)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.35358 (mse_score:  0.354)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30626 (mse_score:  0.306)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.22035 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21708 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19203 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20425 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18690 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20011 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19743 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18055 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19683 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18016 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19579 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17853 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19427 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17777 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19406 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17855 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19395 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17654 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19354 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17844 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19356 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17775 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19392 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17635 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19257 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17628 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19320 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17546 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19247 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17624 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19290 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17555 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19313 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17599 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19333 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 92, with loss  0.19192 compared to final loss  0.19333\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_2\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64621 (mse_score:  0.646)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64293 (mse_score:  0.643)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.38953 (mse_score:  0.390)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38284 (mse_score:  0.383)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.23821 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23475 (mse_score:  0.235)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20193 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20061 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19508 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19435 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19232 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19127 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19108 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18975 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18979 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18742 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18889 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18696 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18938 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18591 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18779 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18529 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18845 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18505 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18657 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18496 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18734 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18491 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18613 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18411 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18617 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18388 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18655 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18351 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18586 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18348 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18670 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18276 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18514 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18239 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_3\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.67980 (mse_score:  0.680)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.72021 (mse_score:  0.720)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.42852 (mse_score:  0.429)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48912 (mse_score:  0.489)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.28548 (mse_score:  0.285)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34490 (mse_score:  0.345)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22855 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27768 (mse_score:  0.278)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20665 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24374 (mse_score:  0.244)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19665 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22736 (mse_score:  0.227)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19188 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21761 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18758 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21161 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18581 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20797 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18515 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20542 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18410 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20357 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18401 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20284 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18226 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20097 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18249 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19854 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18206 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19855 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18249 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19778 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18075 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19720 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18082 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19731 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18076 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19636 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18103 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19704 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.19629 compared to final loss  0.19704\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_4\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65357 (mse_score:  0.654)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64071 (mse_score:  0.641)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.45728 (mse_score:  0.457)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42858 (mse_score:  0.429)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.30850 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28863 (mse_score:  0.289)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23695 (mse_score:  0.237)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23011 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21337 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21226 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20461 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20524 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19874 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19853 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19308 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19409 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18989 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19204 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18866 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18641 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18989 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18509 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18932 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18569 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18987 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18440 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18923 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18379 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18925 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18382 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18868 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18305 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18886 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18280 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18871 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18274 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18787 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18299 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18769 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.18763 compared to final loss  0.18769\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_5\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60485 (mse_score:  0.605)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.68132 (mse_score:  0.681)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.40313 (mse_score:  0.403)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44133 (mse_score:  0.441)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27516 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28218 (mse_score:  0.282)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22427 (mse_score:  0.224)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21154 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20644 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18803 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19812 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17915 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19372 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17621 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17484 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18935 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17435 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18627 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17356 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18518 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17394 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18478 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17406 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18507 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17387 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17433 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18458 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17425 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18298 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17518 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18304 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17541 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18183 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17506 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18199 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17424 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18229 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17425 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 48, with loss  0.17332 compared to final loss  0.17425\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_6\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68145 (mse_score:  0.681)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67752 (mse_score:  0.678)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.49858 (mse_score:  0.499)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48813 (mse_score:  0.488)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.35841 (mse_score:  0.358)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35058 (mse_score:  0.351)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.27419 (mse_score:  0.274)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26919 (mse_score:  0.269)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22849 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22593 (mse_score:  0.226)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20682 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20348 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19807 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19233 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19282 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18632 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19028 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18277 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18845 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17966 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18637 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17826 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18602 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17585 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18612 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17629 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18509 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17495 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18493 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17421 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18561 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17395 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18407 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17314 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18532 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17365 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18374 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17311 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18473 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17276 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 98, with loss  0.17238 compared to final loss  0.17276\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_7\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.54004 (mse_score:  0.540)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.49468 (mse_score:  0.495)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.32739 (mse_score:  0.327)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31340 (mse_score:  0.313)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.22544 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23581 (mse_score:  0.236)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19658 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21712 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18879 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21093 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18498 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20567 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18212 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20350 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17999 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20156 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17947 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19937 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17773 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19937 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17718 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19947 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17667 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19969 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17597 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20038 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17613 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19990 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17576 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19988 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17618 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19910 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17476 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19982 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17620 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19935 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17537 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19890 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17533 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19828 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_8\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.52993 (mse_score:  0.530)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.49096 (mse_score:  0.491)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.30649 (mse_score:  0.306)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29120 (mse_score:  0.291)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.19804 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20503 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.18151 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19418 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.17912 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19358 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.17866 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19238 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17682 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19184 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17622 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19182 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17546 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19161 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17529 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19105 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17590 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19138 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17514 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19184 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17609 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19167 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17463 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19088 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17513 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17501 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19177 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17462 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19110 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17535 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19148 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17620 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19134 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17447 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19104 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 73, with loss  0.19066 compared to final loss  0.19104\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_9\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65910 (mse_score:  0.659)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.60888 (mse_score:  0.609)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.43182 (mse_score:  0.432)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38915 (mse_score:  0.389)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26528 (mse_score:  0.265)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23923 (mse_score:  0.239)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19806 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18474 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18534 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17611 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18220 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17401 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18240 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17259 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18218 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17272 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18256 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17248 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18188 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17207 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18243 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17221 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18120 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17186 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18187 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17227 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18211 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17226 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18181 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17196 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18186 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17186 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18179 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17230 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18176 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17180 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18195 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17201 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18134 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17127 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 91, with loss  0.17121 compared to final loss  0.17127\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_10\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.66294 (mse_score:  0.663)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.73599 (mse_score:  0.736)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.48760 (mse_score:  0.488)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54297 (mse_score:  0.543)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.34773 (mse_score:  0.348)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.39015 (mse_score:  0.390)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.26240 (mse_score:  0.262)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29535 (mse_score:  0.295)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21883 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24543 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19668 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22228 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18708 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20973 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18185 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20306 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17921 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19934 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17742 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19786 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17669 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19602 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17770 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19576 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17576 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19544 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17508 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19491 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17564 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19414 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17524 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19371 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17493 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19382 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17483 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19452 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17437 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19361 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17399 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19389 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 84, with loss  0.19345 compared to final loss  0.19389\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_11\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75391 (mse_score:  0.754)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.72633 (mse_score:  0.726)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.58064 (mse_score:  0.581)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56386 (mse_score:  0.564)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.45286 (mse_score:  0.453)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.43308 (mse_score:  0.433)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.35386 (mse_score:  0.354)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33811 (mse_score:  0.338)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.28745 (mse_score:  0.287)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27352 (mse_score:  0.274)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.24608 (mse_score:  0.246)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23580 (mse_score:  0.236)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.22161 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21491 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.20917 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20264 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19956 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19565 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19465 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19182 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19135 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18922 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19104 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18792 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18836 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18578 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18677 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18468 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18603 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18389 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18576 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18347 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18605 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18284 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18487 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18305 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18486 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18213 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18467 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18214 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.18182 compared to final loss  0.18214\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_12\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.70476 (mse_score:  0.705)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67331 (mse_score:  0.673)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.49931 (mse_score:  0.499)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46779 (mse_score:  0.468)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.34166 (mse_score:  0.342)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.32105 (mse_score:  0.321)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25272 (mse_score:  0.253)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24099 (mse_score:  0.241)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21640 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20817 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20335 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19551 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19702 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18882 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19272 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18614 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19161 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18561 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18983 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18342 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18739 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18285 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18713 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18172 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18595 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18019 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18601 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18008 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18521 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17937 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18443 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18001 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18504 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17979 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18477 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17946 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18444 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17924 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18420 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17891 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.17818 compared to final loss  0.17891\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_13\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60858 (mse_score:  0.609)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.68524 (mse_score:  0.685)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.44237 (mse_score:  0.442)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48376 (mse_score:  0.484)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.32790 (mse_score:  0.328)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34094 (mse_score:  0.341)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25074 (mse_score:  0.251)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24550 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21297 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19832 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19821 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18100 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19291 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17459 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18983 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17115 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18852 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16988 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18755 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16883 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18732 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16834 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18689 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16777 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18571 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16604 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18525 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16612 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18465 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16664 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18543 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16609 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18423 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16622 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18470 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16531 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18404 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16582 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18353 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16604 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 98, with loss  0.16513 compared to final loss  0.16604\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_14\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.72497 (mse_score:  0.725)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.72532 (mse_score:  0.725)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.55108 (mse_score:  0.551)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56228 (mse_score:  0.562)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.44005 (mse_score:  0.440)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.45416 (mse_score:  0.454)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.35923 (mse_score:  0.359)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37628 (mse_score:  0.376)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.30067 (mse_score:  0.301)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31906 (mse_score:  0.319)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.26084 (mse_score:  0.261)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28189 (mse_score:  0.282)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.23148 (mse_score:  0.231)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25585 (mse_score:  0.256)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.21407 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24025 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.20136 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22931 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19524 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22120 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18974 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21728 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18586 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21450 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18315 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21160 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18197 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21062 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17923 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20934 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17848 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20833 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17819 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20832 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17786 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20690 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17726 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20640 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17623 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20572 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_15\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.52237 (mse_score:  0.522)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.53059 (mse_score:  0.531)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.32942 (mse_score:  0.329)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.32743 (mse_score:  0.327)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24329 (mse_score:  0.243)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23218 (mse_score:  0.232)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20385 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19190 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19189 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18053 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18831 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17727 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18710 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17629 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18602 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17571 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18630 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17552 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18659 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17501 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18592 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17514 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18509 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17458 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18658 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17415 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18523 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17425 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18500 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17440 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18470 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17448 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18495 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17341 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18564 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17387 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18527 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17350 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18443 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17387 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.17337 compared to final loss  0.17387\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_16\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.81799 (mse_score:  0.818)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.74041 (mse_score:  0.740)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.64071 (mse_score:  0.641)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.58070 (mse_score:  0.581)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.44549 (mse_score:  0.445)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41787 (mse_score:  0.418)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.30665 (mse_score:  0.307)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30103 (mse_score:  0.301)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.23565 (mse_score:  0.236)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24498 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20798 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21950 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19519 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20764 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18975 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20003 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18653 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19583 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18539 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19418 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18441 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19132 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18296 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18977 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18194 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18866 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18194 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18794 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18133 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18696 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18118 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18755 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18137 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18668 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18103 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18605 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18121 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18567 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18149 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18532 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_17\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.72144 (mse_score:  0.721)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.68393 (mse_score:  0.684)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.54585 (mse_score:  0.546)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.51857 (mse_score:  0.519)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.39341 (mse_score:  0.393)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37162 (mse_score:  0.372)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.28200 (mse_score:  0.282)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26773 (mse_score:  0.268)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22662 (mse_score:  0.227)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21983 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20567 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20154 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19661 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19226 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19256 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18612 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19037 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18829 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18020 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18756 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17760 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18644 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17663 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18552 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17548 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18554 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17450 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18407 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17369 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18449 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17299 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18426 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17315 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18338 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17275 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18411 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17281 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18340 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17211 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.17194 compared to final loss  0.17211\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_18\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.63267 (mse_score:  0.633)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56413 (mse_score:  0.564)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.41280 (mse_score:  0.413)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.36947 (mse_score:  0.369)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.25378 (mse_score:  0.254)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23686 (mse_score:  0.237)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19245 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19129 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18381 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18567 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18519 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18206 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18454 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18245 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18481 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18204 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18402 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18150 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18385 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18059 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18460 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18077 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18465 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18062 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18365 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18100 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18386 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18015 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18356 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18113 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18329 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18131 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18320 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18086 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18357 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18055 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18358 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18021 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18280 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.18267 compared to final loss  0.18280\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_19\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.70279 (mse_score:  0.703)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67931 (mse_score:  0.679)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.51519 (mse_score:  0.515)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.49840 (mse_score:  0.498)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.36935 (mse_score:  0.369)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35250 (mse_score:  0.353)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.27021 (mse_score:  0.270)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26545 (mse_score:  0.265)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22185 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22802 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20252 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21391 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19294 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20840 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18863 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20500 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18547 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20425 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18430 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20294 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18351 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20104 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18227 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20174 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18170 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20086 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18129 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20077 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18077 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20025 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18069 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20091 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17984 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20000 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18075 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20020 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17979 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20060 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17968 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20017 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 91, with loss  0.19957 compared to final loss  0.20017\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_20\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.51373 (mse_score:  0.514)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44360 (mse_score:  0.444)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.30955 (mse_score:  0.310)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26163 (mse_score:  0.262)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.21680 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18798 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19792 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17711 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19271 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17285 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19174 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17111 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19002 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16981 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18957 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16853 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18887 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16816 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18841 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16815 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18797 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16759 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18791 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16618 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18797 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16718 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18726 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16664 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18707 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16615 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18741 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16631 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18834 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16671 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18682 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16552 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18790 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16600 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18676 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16566 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 90, with loss  0.16552 compared to final loss  0.16566\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_21\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.47143 (mse_score:  0.471)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.45751 (mse_score:  0.458)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.26210 (mse_score:  0.262)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26640 (mse_score:  0.266)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.19947 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21567 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.18974 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20849 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18482 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20446 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18376 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20229 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18040 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20109 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18031 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19984 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17909 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19953 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17910 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19898 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17806 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19841 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17824 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19859 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17780 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19837 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17721 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19812 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17745 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19756 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17606 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19713 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17697 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19766 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17604 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19778 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17663 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19734 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17609 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19711 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 89, with loss  0.19690 compared to final loss  0.19711\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_22\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.66250 (mse_score:  0.662)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.65730 (mse_score:  0.657)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.43858 (mse_score:  0.439)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42251 (mse_score:  0.423)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26604 (mse_score:  0.266)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25213 (mse_score:  0.252)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20150 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19778 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19037 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19092 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18552 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18895 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18479 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18751 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18325 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18709 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18221 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18687 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18157 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18700 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18023 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18572 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17950 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18622 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17935 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18506 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17931 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18502 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17989 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18504 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17870 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18517 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17886 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18559 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17793 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18490 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17832 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18507 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17805 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18548 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 82, with loss  0.18480 compared to final loss  0.18548\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_23\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.47911 (mse_score:  0.479)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38384 (mse_score:  0.384)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.27649 (mse_score:  0.276)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23354 (mse_score:  0.234)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.20340 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20350 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19038 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19856 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18684 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19331 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18465 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18968 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18358 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18800 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18220 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18673 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18305 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18663 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18220 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18616 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18169 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18414 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18190 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18529 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18026 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18445 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18102 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18397 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18074 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18430 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18012 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18330 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18021 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18361 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18027 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18386 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18028 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18054 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18301 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 92, with loss  0.18299 compared to final loss  0.18301\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_24\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.63214 (mse_score:  0.632)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.50247 (mse_score:  0.502)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.36715 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26076 (mse_score:  0.261)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.23787 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16974 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20935 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15700 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20109 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15420 (mse_score:  0.154)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19709 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15288 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19495 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15348 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19448 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15267 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19380 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15244 (mse_score:  0.152)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19296 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15296 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19173 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15304 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19166 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15266 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19157 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15312 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19014 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15348 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19020 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15392 (mse_score:  0.154)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18883 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15319 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18918 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15309 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18976 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15368 (mse_score:  0.154)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18922 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15345 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18847 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15356 (mse_score:  0.154)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 52, with loss  0.15216 compared to final loss  0.15356\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_25\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.57283 (mse_score:  0.573)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62910 (mse_score:  0.629)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.35409 (mse_score:  0.354)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40399 (mse_score:  0.404)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.23834 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28881 (mse_score:  0.289)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20106 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24574 (mse_score:  0.246)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18655 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22579 (mse_score:  0.226)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18209 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21724 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17845 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21134 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17483 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20838 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17456 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20699 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17351 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20638 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17328 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20531 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17344 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20472 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17288 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20441 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17161 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20412 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17146 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20434 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17125 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20404 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17113 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20409 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17126 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20484 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17045 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20462 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17070 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20380 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 96, with loss  0.20335 compared to final loss  0.20380\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_26\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.61110 (mse_score:  0.611)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67897 (mse_score:  0.679)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.41561 (mse_score:  0.416)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46591 (mse_score:  0.466)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27634 (mse_score:  0.276)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31309 (mse_score:  0.313)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21035 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23516 (mse_score:  0.235)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18747 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20830 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18191 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19968 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18011 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19770 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17991 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19587 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17909 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19475 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17817 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19468 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17744 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19416 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17780 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19278 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17710 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19238 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17683 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19335 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17676 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19180 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17588 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19183 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17666 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19208 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17634 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19131 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17688 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19230 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17502 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19166 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 94, with loss  0.19116 compared to final loss  0.19166\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_27\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64108 (mse_score:  0.641)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.61408 (mse_score:  0.614)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.43795 (mse_score:  0.438)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41642 (mse_score:  0.416)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.30286 (mse_score:  0.303)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28876 (mse_score:  0.289)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24039 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22875 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21578 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20397 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20137 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19259 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19605 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18713 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19171 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18247 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18925 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18038 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18800 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18027 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18616 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17965 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18621 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17823 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18559 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17896 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18505 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17842 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18415 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17873 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18497 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17817 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18322 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17813 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18413 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17802 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18375 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17822 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18451 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17911 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.17755 compared to final loss  0.17911\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_28\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.59098 (mse_score:  0.591)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.49526 (mse_score:  0.495)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.37260 (mse_score:  0.373)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30360 (mse_score:  0.304)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26650 (mse_score:  0.267)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21610 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21932 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18079 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20340 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16599 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19501 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16076 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19418 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15902 (mse_score:  0.159)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19113 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15835 (mse_score:  0.158)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19125 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15721 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19035 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15798 (mse_score:  0.158)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18981 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15708 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18974 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15693 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18905 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15703 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18809 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15677 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18920 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15585 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18899 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15586 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18878 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15665 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18844 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15621 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18896 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15585 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18896 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15617 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 84, with loss  0.15578 compared to final loss  0.15617\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_29\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.53461 (mse_score:  0.535)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56133 (mse_score:  0.561)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.33594 (mse_score:  0.336)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33992 (mse_score:  0.340)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.22808 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22453 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19702 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18924 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19023 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18098 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18766 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17737 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18610 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17610 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18513 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17508 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18489 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17409 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18341 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17312 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18285 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17257 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18337 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17233 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18300 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17308 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18273 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17229 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17279 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17205 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18200 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17176 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18251 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17143 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18222 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17209 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18154 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17105 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 96, with loss  0.17101 compared to final loss  0.17105\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_30\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.81298 (mse_score:  0.813)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.89646 (mse_score:  0.896)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.59736 (mse_score:  0.597)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.65183 (mse_score:  0.652)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.37686 (mse_score:  0.377)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41514 (mse_score:  0.415)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24493 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27956 (mse_score:  0.280)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19686 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22819 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18566 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21439 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18194 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20865 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17954 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20653 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17920 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20393 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17776 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20311 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17800 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20231 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17681 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20100 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17682 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20067 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17699 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20180 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17567 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20077 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17725 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19997 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17600 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20026 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17608 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19982 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17574 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19870 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17601 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19986 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 95, with loss  0.19870 compared to final loss  0.19986\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_31\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68101 (mse_score:  0.681)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.66285 (mse_score:  0.663)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.41298 (mse_score:  0.413)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38273 (mse_score:  0.383)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24420 (mse_score:  0.244)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21780 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19864 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17564 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19323 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16951 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19050 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16734 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19070 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16688 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18981 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16573 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18926 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16484 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18831 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16432 (mse_score:  0.164)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18715 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16367 (mse_score:  0.164)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18714 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16296 (mse_score:  0.163)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18701 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16233 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18702 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16167 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18691 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16159 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18653 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16142 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18698 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16142 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18680 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16171 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18647 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16124 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18623 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16039 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_32\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65543 (mse_score:  0.655)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67553 (mse_score:  0.676)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.45373 (mse_score:  0.454)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48647 (mse_score:  0.486)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.33894 (mse_score:  0.339)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37929 (mse_score:  0.379)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.27385 (mse_score:  0.274)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31195 (mse_score:  0.312)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.23492 (mse_score:  0.235)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26693 (mse_score:  0.267)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.21093 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23826 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19635 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22168 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18991 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21132 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18478 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20539 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18184 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20078 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18047 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19806 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17999 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19592 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17894 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19553 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17898 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19488 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17857 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19416 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17736 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19349 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17808 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19282 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17767 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19293 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17791 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19283 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17706 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19239 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.19228 compared to final loss  0.19239\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_33\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75879 (mse_score:  0.759)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.81743 (mse_score:  0.817)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.58818 (mse_score:  0.588)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.63080 (mse_score:  0.631)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.41613 (mse_score:  0.416)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44294 (mse_score:  0.443)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.28577 (mse_score:  0.286)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30412 (mse_score:  0.304)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21949 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23296 (mse_score:  0.233)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19596 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20681 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18674 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19731 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18453 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19321 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18223 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19040 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18091 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18935 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18032 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18798 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17998 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18811 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17951 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18732 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17932 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18661 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17908 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18596 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17864 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18629 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17924 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18614 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17876 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18610 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17777 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18642 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17896 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18570 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 88, with loss  0.18544 compared to final loss  0.18570\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_34\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.86086 (mse_score:  0.861)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.79295 (mse_score:  0.793)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.69417 (mse_score:  0.694)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.61732 (mse_score:  0.617)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.53519 (mse_score:  0.535)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46039 (mse_score:  0.460)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.40457 (mse_score:  0.405)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33842 (mse_score:  0.338)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.30968 (mse_score:  0.310)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26381 (mse_score:  0.264)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.25128 (mse_score:  0.251)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22453 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.22023 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20691 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.20631 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19976 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19998 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19614 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19422 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19428 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19096 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19246 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18909 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19176 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18748 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19215 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18599 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19116 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18473 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19052 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18533 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19066 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18459 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19062 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18380 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19060 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18324 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19094 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18383 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18964 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_35\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.79468 (mse_score:  0.795)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.73604 (mse_score:  0.736)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.63653 (mse_score:  0.637)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.59171 (mse_score:  0.592)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.44523 (mse_score:  0.445)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42588 (mse_score:  0.426)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.30464 (mse_score:  0.305)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30050 (mse_score:  0.301)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.23363 (mse_score:  0.234)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23720 (mse_score:  0.237)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20680 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21017 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19572 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19875 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18909 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19390 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18707 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19035 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18391 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18829 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18345 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18686 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18509 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18266 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18464 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18143 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18527 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18027 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18414 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18091 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18417 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18043 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18444 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17977 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18444 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18029 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18372 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17937 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18456 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 92, with loss  0.18349 compared to final loss  0.18456\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_36\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.51296 (mse_score:  0.513)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55973 (mse_score:  0.560)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.29428 (mse_score:  0.294)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30927 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.21299 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21357 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19013 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18704 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18524 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18181 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18227 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17953 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18097 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17976 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18095 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18006 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17919 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18014 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17986 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18094 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17896 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18080 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17987 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18137 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17839 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18135 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17847 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18098 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17875 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18048 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17861 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18118 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17775 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18159 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17842 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18153 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17806 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18138 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17826 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18127 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 28, with loss  0.17934 compared to final loss  0.18127\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_37\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.73106 (mse_score:  0.731)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62425 (mse_score:  0.624)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.50941 (mse_score:  0.509)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42880 (mse_score:  0.429)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.32569 (mse_score:  0.326)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27248 (mse_score:  0.272)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22510 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18654 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19759 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16390 (mse_score:  0.164)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19319 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15940 (mse_score:  0.159)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19263 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15858 (mse_score:  0.159)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19220 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15847 (mse_score:  0.158)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19048 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15763 (mse_score:  0.158)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18989 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15723 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18993 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15697 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18968 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15703 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18883 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15620 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18901 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15639 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18797 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15707 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18813 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15692 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18803 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15709 (mse_score:  0.157)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18763 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15646 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18727 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15754 (mse_score:  0.158)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18805 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15639 (mse_score:  0.156)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 89, with loss  0.15612 compared to final loss  0.15639\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_38\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75449 (mse_score:  0.754)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.69805 (mse_score:  0.698)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.54890 (mse_score:  0.549)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.50063 (mse_score:  0.501)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.36691 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33701 (mse_score:  0.337)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.26144 (mse_score:  0.261)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24847 (mse_score:  0.248)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22177 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21963 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20832 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20846 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.20137 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20248 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19755 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19965 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19368 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19753 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19192 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19643 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19112 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19613 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19045 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19543 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18880 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19390 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18793 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19376 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18638 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19356 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18716 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19293 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18599 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19314 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18618 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19256 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18506 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19233 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18616 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19188 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.19150 compared to final loss  0.19188\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_39\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.78398 (mse_score:  0.784)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.67252 (mse_score:  0.673)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.55746 (mse_score:  0.557)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.47235 (mse_score:  0.472)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.35500 (mse_score:  0.355)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30407 (mse_score:  0.304)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24012 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22071 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20155 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19641 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19278 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19106 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18837 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18817 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18575 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18740 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18532 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18641 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18361 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18551 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18360 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18522 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18278 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18477 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18375 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18444 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18215 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18452 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18211 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18380 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18202 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18316 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18125 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18397 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18184 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18293 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18118 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18419 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18104 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18351 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.18252 compared to final loss  0.18351\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_40\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.57565 (mse_score:  0.576)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.57929 (mse_score:  0.579)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39896 (mse_score:  0.399)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40822 (mse_score:  0.408)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.31015 (mse_score:  0.310)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30969 (mse_score:  0.310)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25761 (mse_score:  0.258)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25325 (mse_score:  0.253)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22642 (mse_score:  0.226)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21665 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20907 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19464 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19846 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18338 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19348 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17600 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19124 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17244 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18890 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17008 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18887 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16878 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18737 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16727 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18747 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16620 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18623 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16586 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18696 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16532 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18587 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16551 (mse_score:  0.166)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18631 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16486 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18601 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16461 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18594 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16425 (mse_score:  0.164)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18574 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16505 (mse_score:  0.165)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.16398 compared to final loss  0.16505\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_41\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60568 (mse_score:  0.606)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.50508 (mse_score:  0.505)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39732 (mse_score:  0.397)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33821 (mse_score:  0.338)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27721 (mse_score:  0.277)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25117 (mse_score:  0.251)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22871 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21419 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20646 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19662 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19653 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18617 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19081 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18083 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18682 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17676 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18565 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17646 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18363 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17530 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18433 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17447 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18318 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17491 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18292 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17399 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18296 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17438 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18414 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17338 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18191 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17425 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18277 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17396 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18233 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17385 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18237 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17370 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18244 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17352 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 87, with loss  0.17268 compared to final loss  0.17352\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_42\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68000 (mse_score:  0.680)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.71847 (mse_score:  0.718)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.45979 (mse_score:  0.460)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.50046 (mse_score:  0.500)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.33091 (mse_score:  0.331)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35474 (mse_score:  0.355)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25914 (mse_score:  0.259)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27046 (mse_score:  0.270)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22808 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22723 (mse_score:  0.227)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.21280 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20546 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.20421 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19273 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19930 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18593 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19649 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18258 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19496 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17891 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19276 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17724 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19346 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17606 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19108 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17501 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19020 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17368 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18995 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17300 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18963 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17236 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18862 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17215 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18846 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17063 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18867 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17071 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18873 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17141 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.17062 compared to final loss  0.17141\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_43\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.61232 (mse_score:  0.612)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62796 (mse_score:  0.628)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.40713 (mse_score:  0.407)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42000 (mse_score:  0.420)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27468 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29827 (mse_score:  0.298)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21889 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24240 (mse_score:  0.242)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19531 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21813 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18596 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20923 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18120 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20433 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17956 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20081 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17749 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19834 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17621 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19789 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17582 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19811 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17526 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19651 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17423 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19684 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17504 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19677 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17461 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19609 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17508 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19572 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17287 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19579 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17405 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19575 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17308 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19565 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17440 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19548 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 92, with loss  0.19519 compared to final loss  0.19548\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_44\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.61025 (mse_score:  0.610)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46677 (mse_score:  0.467)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39838 (mse_score:  0.398)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31157 (mse_score:  0.312)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.28853 (mse_score:  0.289)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23510 (mse_score:  0.235)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23483 (mse_score:  0.235)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19943 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21098 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18442 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19925 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17722 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19399 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17316 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18967 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17136 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18815 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16895 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18722 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16781 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18686 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16784 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18585 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16820 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18549 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16820 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18466 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16774 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18532 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16759 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18496 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16799 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18522 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16745 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18577 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16793 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18567 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16718 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18408 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16713 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 83, with loss  0.16711 compared to final loss  0.16713\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_45\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64340 (mse_score:  0.643)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.66771 (mse_score:  0.668)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.43177 (mse_score:  0.432)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42935 (mse_score:  0.429)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27658 (mse_score:  0.277)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25974 (mse_score:  0.260)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21350 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19307 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19569 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17650 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19077 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17188 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18882 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17051 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18674 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17031 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18597 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17026 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18606 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16937 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18541 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16992 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18536 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16976 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18459 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16977 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18369 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16953 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18389 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16962 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18516 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16942 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18365 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16901 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18405 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16973 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16943 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18424 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16961 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 78, with loss  0.16893 compared to final loss  0.16961\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_46\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60888 (mse_score:  0.609)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56014 (mse_score:  0.560)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39794 (mse_score:  0.398)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.36411 (mse_score:  0.364)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26637 (mse_score:  0.266)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25001 (mse_score:  0.250)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20807 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20468 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19115 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19085 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18658 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18649 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18434 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18262 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18288 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18131 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18223 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17995 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18110 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17880 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18205 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17801 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18115 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17793 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18081 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17740 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18169 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17677 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18073 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17766 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18172 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17681 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18079 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17661 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17988 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17737 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18076 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17618 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18128 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17681 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.17601 compared to final loss  0.17681\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_47\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.66068 (mse_score:  0.661)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62475 (mse_score:  0.625)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.45249 (mse_score:  0.452)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42036 (mse_score:  0.420)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29340 (mse_score:  0.293)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27665 (mse_score:  0.277)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22188 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21757 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19628 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19645 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18765 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18881 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18347 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18556 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18224 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18503 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17939 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18466 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17952 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18501 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17873 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18447 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17889 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18450 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17868 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18487 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17887 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18417 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17750 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18503 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17860 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18530 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17682 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18543 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17680 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18518 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17698 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18481 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17834 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18540 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 46, with loss  0.18390 compared to final loss  0.18540\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_48\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68564 (mse_score:  0.686)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.69622 (mse_score:  0.696)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.44861 (mse_score:  0.449)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44108 (mse_score:  0.441)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27534 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26515 (mse_score:  0.265)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20883 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20172 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19666 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19036 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19186 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18691 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19001 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18489 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18670 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18259 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18468 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18162 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18390 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18128 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18184 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18062 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18228 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18018 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18173 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18009 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18171 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18055 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18114 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17960 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18025 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18019 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18051 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17958 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18003 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17944 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18004 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17996 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18024 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17968 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 92, with loss  0.17839 compared to final loss  0.17968\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_49\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.73503 (mse_score:  0.735)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.76827 (mse_score:  0.768)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.57992 (mse_score:  0.580)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.61239 (mse_score:  0.612)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.41270 (mse_score:  0.413)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.43865 (mse_score:  0.439)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.28198 (mse_score:  0.282)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30777 (mse_score:  0.308)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21676 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24264 (mse_score:  0.243)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19171 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21775 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18440 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20794 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18226 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20441 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18194 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20271 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18070 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20167 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17905 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20066 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17962 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20034 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17908 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20019 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17857 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19934 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17833 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19908 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17879 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19832 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17858 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19868 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17800 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19823 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17759 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19834 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17812 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19854 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.19778 compared to final loss  0.19854\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_50\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.80789 (mse_score:  0.808)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.80020 (mse_score:  0.800)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.58503 (mse_score:  0.585)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.57000 (mse_score:  0.570)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.41053 (mse_score:  0.411)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.39439 (mse_score:  0.394)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.29834 (mse_score:  0.298)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28961 (mse_score:  0.290)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.24883 (mse_score:  0.249)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24373 (mse_score:  0.244)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.22886 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22481 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.21776 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21455 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.20947 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20691 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.20419 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20102 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.20215 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19750 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19859 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19475 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19522 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19318 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19313 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19018 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19201 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18969 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19217 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18893 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.19203 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18777 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.19067 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18675 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18690 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18881 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18632 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18928 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18559 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.18553 compared to final loss  0.18559\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_51\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.57096 (mse_score:  0.571)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.60836 (mse_score:  0.608)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.36660 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.39649 (mse_score:  0.396)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24141 (mse_score:  0.241)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27183 (mse_score:  0.272)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19891 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22666 (mse_score:  0.227)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18546 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21065 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18062 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20556 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17818 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20202 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17725 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20028 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17743 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19992 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17730 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19870 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17638 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19822 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17607 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19807 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17544 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19869 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17472 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19705 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17423 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19646 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17472 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19677 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17520 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19654 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17458 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19606 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17434 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19603 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17445 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19576 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 96, with loss  0.19496 compared to final loss  0.19576\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_52\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68065 (mse_score:  0.681)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.74757 (mse_score:  0.748)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.48283 (mse_score:  0.483)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.56077 (mse_score:  0.561)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.33115 (mse_score:  0.331)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40588 (mse_score:  0.406)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24200 (mse_score:  0.242)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29627 (mse_score:  0.296)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20160 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23802 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18953 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21394 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18489 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20586 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18345 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20193 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18236 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20086 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18122 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19888 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18074 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19795 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17955 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19776 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18019 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19695 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17939 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19558 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17861 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19482 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17950 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19435 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17934 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19447 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17884 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19433 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17857 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19446 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17807 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19337 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_53\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64651 (mse_score:  0.647)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.77102 (mse_score:  0.771)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.46648 (mse_score:  0.466)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55213 (mse_score:  0.552)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29493 (mse_score:  0.295)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34488 (mse_score:  0.345)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21287 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23854 (mse_score:  0.239)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19080 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20365 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18526 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19278 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18316 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18823 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18208 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18717 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18176 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18529 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18170 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18386 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18181 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18265 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18143 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18256 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17980 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18181 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17999 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18176 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17965 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18115 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17957 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18111 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17957 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18090 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18021 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18013 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17919 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18009 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17906 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18084 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 87, with loss  0.17971 compared to final loss  0.18084\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_54\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64572 (mse_score:  0.646)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.57119 (mse_score:  0.571)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.42715 (mse_score:  0.427)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.36743 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.25806 (mse_score:  0.258)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22473 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20112 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18562 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19023 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17953 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18471 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17766 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18443 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17796 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18352 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17729 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18417 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17742 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18385 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17710 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18247 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17737 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18216 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17627 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18232 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17688 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18214 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17650 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18189 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17596 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18304 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17649 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18286 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17604 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18210 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17554 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18203 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17637 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18204 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17550 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.17526 compared to final loss  0.17550\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_55\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.62527 (mse_score:  0.625)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.57927 (mse_score:  0.579)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.40288 (mse_score:  0.403)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37709 (mse_score:  0.377)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24921 (mse_score:  0.249)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24534 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19938 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21027 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18984 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20435 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18732 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20044 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18541 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19861 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18386 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19642 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18221 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19451 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18122 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19351 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17946 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19184 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18081 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19246 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17944 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19224 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17886 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19084 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17933 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19065 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17866 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19041 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17883 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19016 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17821 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18968 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17838 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18948 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17767 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18873 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_56\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.69612 (mse_score:  0.696)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.53465 (mse_score:  0.535)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.46487 (mse_score:  0.465)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34820 (mse_score:  0.348)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.31924 (mse_score:  0.319)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24314 (mse_score:  0.243)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24665 (mse_score:  0.247)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19704 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21421 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17896 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20132 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17400 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19417 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17243 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19131 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17161 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18896 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17126 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18779 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17178 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18826 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17095 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18572 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17044 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18676 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17142 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18634 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17044 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18601 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17014 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18604 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17073 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18535 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17031 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18558 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16990 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18569 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16962 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18506 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17033 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 98, with loss  0.16961 compared to final loss  0.17033\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_57\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.62794 (mse_score:  0.628)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55782 (mse_score:  0.558)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.40552 (mse_score:  0.406)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34858 (mse_score:  0.349)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.27108 (mse_score:  0.271)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23734 (mse_score:  0.237)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21490 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19684 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19663 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18561 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19103 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18336 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18856 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18135 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18759 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18171 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18630 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18071 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18541 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18060 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18585 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18054 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18382 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17994 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18385 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17979 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18306 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17964 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18411 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17945 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18370 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17961 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18394 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17947 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18202 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17902 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18315 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17871 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17937 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 95, with loss  0.17871 compared to final loss  0.17937\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_58\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.55869 (mse_score:  0.559)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55174 (mse_score:  0.552)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.36689 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34194 (mse_score:  0.342)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24630 (mse_score:  0.246)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22143 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20909 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18774 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20064 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17945 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19691 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17604 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19619 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17463 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19587 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17389 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19448 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17348 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19359 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17360 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19273 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17242 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19228 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17130 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19275 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17158 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19096 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17118 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19079 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17131 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.19126 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17061 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18916 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17021 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.19013 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17055 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18921 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16955 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18897 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16932 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_59\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.70381 (mse_score:  0.704)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64189 (mse_score:  0.642)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.53603 (mse_score:  0.536)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.47339 (mse_score:  0.473)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.42076 (mse_score:  0.421)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35796 (mse_score:  0.358)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.35521 (mse_score:  0.355)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29180 (mse_score:  0.292)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.30926 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24509 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.27482 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21382 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.25009 (mse_score:  0.250)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18967 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.23387 (mse_score:  0.234)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17299 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.22192 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16154 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.21413 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15464 (mse_score:  0.155)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.20815 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14955 (mse_score:  0.150)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.20503 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14645 (mse_score:  0.146)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.20223 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14451 (mse_score:  0.145)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.20050 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14285 (mse_score:  0.143)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19920 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14208 (mse_score:  0.142)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.19763 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14080 (mse_score:  0.141)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.19822 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14050 (mse_score:  0.141)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.19785 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14063 (mse_score:  0.141)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.19692 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14040 (mse_score:  0.140)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.19664 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.13997 (mse_score:  0.140)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 89, with loss  0.13960 compared to final loss  0.13997\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_60\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.67592 (mse_score:  0.676)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62923 (mse_score:  0.629)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.46236 (mse_score:  0.462)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41767 (mse_score:  0.418)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.32062 (mse_score:  0.321)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28335 (mse_score:  0.283)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24395 (mse_score:  0.244)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22029 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21380 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19597 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19992 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18686 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19378 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18237 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18922 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17921 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18814 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17843 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18656 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17755 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18487 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17617 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18467 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17571 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18421 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17559 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18399 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17489 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18383 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17496 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18349 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17450 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18269 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17428 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18275 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17531 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18231 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17447 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18170 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17416 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.17374 compared to final loss  0.17416\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_61\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.68304 (mse_score:  0.683)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.68724 (mse_score:  0.687)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.48342 (mse_score:  0.483)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48993 (mse_score:  0.490)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.33115 (mse_score:  0.331)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34758 (mse_score:  0.348)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25121 (mse_score:  0.251)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27481 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21782 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24132 (mse_score:  0.241)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20310 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22510 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19552 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21369 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19130 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20782 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18898 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20282 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18522 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19986 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18418 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19719 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18264 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19584 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18237 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19424 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18134 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19258 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18079 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19214 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18029 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19028 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18031 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19026 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17929 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18986 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17954 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18936 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18087 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18852 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_62\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.77526 (mse_score:  0.775)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.87801 (mse_score:  0.878)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.62226 (mse_score:  0.622)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.68424 (mse_score:  0.684)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.44235 (mse_score:  0.442)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.47738 (mse_score:  0.477)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.30649 (mse_score:  0.306)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.32795 (mse_score:  0.328)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22615 (mse_score:  0.226)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24215 (mse_score:  0.242)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19378 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20591 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19457 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17788 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19020 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17737 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18832 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17642 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18756 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17676 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18715 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17590 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18701 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17679 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18688 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17544 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18685 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17656 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18729 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17550 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18624 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17653 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18681 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17619 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18698 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17603 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18688 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17619 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18667 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 77, with loss  0.18552 compared to final loss  0.18667\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_63\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.73931 (mse_score:  0.739)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.76401 (mse_score:  0.764)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.58304 (mse_score:  0.583)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.59369 (mse_score:  0.594)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.41741 (mse_score:  0.417)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41222 (mse_score:  0.412)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.29412 (mse_score:  0.294)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28304 (mse_score:  0.283)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22900 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21703 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20412 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19329 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19381 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18403 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19052 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17940 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18699 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17607 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18657 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17392 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18590 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17312 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18480 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17241 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18516 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17203 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18469 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17157 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18484 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17120 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18323 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17122 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18291 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17129 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18391 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17070 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18358 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17069 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18434 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17043 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 81, with loss  0.17037 compared to final loss  0.17043\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_64\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.63016 (mse_score:  0.630)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44858 (mse_score:  0.449)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39728 (mse_score:  0.397)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27735 (mse_score:  0.277)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.25938 (mse_score:  0.259)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19423 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20676 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17413 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19089 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17181 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18700 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17057 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18516 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17058 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18367 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16955 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18342 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16960 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18356 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16908 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18295 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16834 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18298 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16928 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18355 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16917 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18492 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16893 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18347 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16930 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18196 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16919 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16864 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18252 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16861 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18220 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16891 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18350 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16931 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 55, with loss  0.16834 compared to final loss  0.16931\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_65\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.71097 (mse_score:  0.711)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.70327 (mse_score:  0.703)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.49673 (mse_score:  0.497)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48253 (mse_score:  0.483)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.31496 (mse_score:  0.315)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30068 (mse_score:  0.301)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22408 (mse_score:  0.224)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21578 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19647 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19351 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18715 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18944 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18478 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18755 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18272 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18749 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18196 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18724 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18100 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18688 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17963 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18638 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17962 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18670 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17914 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18605 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17895 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18621 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17859 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18638 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17824 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18581 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17807 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18612 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17819 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18597 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17810 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18543 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17736 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18572 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 76, with loss  0.18511 compared to final loss  0.18572\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_66\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.80175 (mse_score:  0.802)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.71345 (mse_score:  0.713)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.63322 (mse_score:  0.633)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55585 (mse_score:  0.556)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.47116 (mse_score:  0.471)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40241 (mse_score:  0.402)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.34330 (mse_score:  0.343)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28818 (mse_score:  0.288)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.27109 (mse_score:  0.271)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22942 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.23648 (mse_score:  0.236)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20206 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.21719 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18956 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.20888 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18287 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.20254 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17974 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19843 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17817 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19719 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17666 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19464 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17610 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19363 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17502 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19259 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17427 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19240 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17408 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.19123 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17389 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.19046 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17336 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.19113 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17386 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18872 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17345 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.19124 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17276 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 98, with loss  0.17256 compared to final loss  0.17276\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_67\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65327 (mse_score:  0.653)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.60547 (mse_score:  0.605)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.44805 (mse_score:  0.448)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.39674 (mse_score:  0.397)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.26747 (mse_score:  0.267)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22999 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19858 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17666 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19022 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17207 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18739 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17068 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18666 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17057 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18558 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16938 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18568 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16937 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18412 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16913 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18378 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16904 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18350 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16930 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18404 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16925 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18253 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16842 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18258 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16895 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18280 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16912 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18368 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16835 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18242 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16937 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18175 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16894 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18237 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16891 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 85, with loss  0.16835 compared to final loss  0.16891\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_68\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.61106 (mse_score:  0.611)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.50913 (mse_score:  0.509)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.36842 (mse_score:  0.368)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30018 (mse_score:  0.300)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.22997 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19824 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19456 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17918 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18905 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17506 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18788 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17344 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18595 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17127 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18597 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17170 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18488 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17064 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18435 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17140 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18347 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17048 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18340 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17075 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18275 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17017 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18402 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16996 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18333 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17029 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18298 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17005 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18243 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16993 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18238 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16984 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18245 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16967 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18175 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16993 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 82, with loss  0.16958 compared to final loss  0.16993\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_69\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.74777 (mse_score:  0.748)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.65767 (mse_score:  0.658)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.53296 (mse_score:  0.533)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46448 (mse_score:  0.464)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.36449 (mse_score:  0.364)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30993 (mse_score:  0.310)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25382 (mse_score:  0.254)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22061 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20412 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18555 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18965 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17717 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18648 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17540 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18468 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17468 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18544 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17431 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18566 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17439 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18455 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17458 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18430 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17410 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18379 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17364 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18364 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17394 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18285 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17336 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18370 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17382 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18331 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17362 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18324 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17348 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18299 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17291 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18325 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17295 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 87, with loss  0.17272 compared to final loss  0.17295\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_70\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.84835 (mse_score:  0.848)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.82218 (mse_score:  0.822)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.65965 (mse_score:  0.660)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64254 (mse_score:  0.643)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.47288 (mse_score:  0.473)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.45772 (mse_score:  0.458)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.30948 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30551 (mse_score:  0.306)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.22038 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22782 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18682 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20575 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17912 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20065 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17690 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20015 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17576 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20003 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17504 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19973 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17469 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20027 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17368 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20045 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17410 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19985 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17421 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20033 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17388 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19960 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17391 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20003 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17359 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19977 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17267 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19975 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17311 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20010 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17398 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20001 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 49, with loss  0.19919 compared to final loss  0.20001\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_71\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75766 (mse_score:  0.758)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.82804 (mse_score:  0.828)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.51080 (mse_score:  0.511)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55731 (mse_score:  0.557)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.32804 (mse_score:  0.328)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.36161 (mse_score:  0.362)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23964 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26215 (mse_score:  0.262)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20758 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22224 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19709 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20754 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19344 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20105 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19204 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19817 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18909 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19538 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18776 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19325 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18803 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19263 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18614 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19125 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18547 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19019 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18566 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18861 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18549 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18907 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18456 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18782 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18377 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18746 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18396 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18725 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18416 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18687 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18408 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18651 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 96, with loss  0.18629 compared to final loss  0.18651\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_72\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.76041 (mse_score:  0.760)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64009 (mse_score:  0.640)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.50596 (mse_score:  0.506)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40119 (mse_score:  0.401)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29999 (mse_score:  0.300)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22527 (mse_score:  0.225)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21189 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16082 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19716 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15283 (mse_score:  0.153)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19556 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15167 (mse_score:  0.152)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19434 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15023 (mse_score:  0.150)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19417 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14935 (mse_score:  0.149)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.19340 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14901 (mse_score:  0.149)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.19195 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14877 (mse_score:  0.149)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.19085 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14823 (mse_score:  0.148)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.19226 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14763 (mse_score:  0.148)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19154 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14762 (mse_score:  0.148)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19115 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14742 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19047 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14730 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18943 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14761 (mse_score:  0.148)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.19058 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14668 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18971 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14693 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18943 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14674 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.19041 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.14703 (mse_score:  0.147)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 81, with loss  0.14650 compared to final loss  0.14703\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_73\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.55110 (mse_score:  0.551)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55418 (mse_score:  0.554)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.35482 (mse_score:  0.355)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34761 (mse_score:  0.348)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24055 (mse_score:  0.241)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22909 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19777 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18830 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18548 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17686 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18368 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17350 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18352 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17310 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18167 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17238 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18234 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17140 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18131 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17109 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18267 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17098 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18176 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17177 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18176 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17063 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18114 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17103 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18200 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17159 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18086 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17128 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18144 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17059 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18150 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17101 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18251 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17125 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18150 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17131 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 85, with loss  0.17059 compared to final loss  0.17131\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_74\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.69972 (mse_score:  0.700)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.71844 (mse_score:  0.718)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.47570 (mse_score:  0.476)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.48066 (mse_score:  0.481)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.30655 (mse_score:  0.307)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30933 (mse_score:  0.309)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22076 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22882 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19287 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20553 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18481 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19922 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18198 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19743 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17932 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19706 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17744 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19612 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17644 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19673 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17546 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19656 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17484 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19648 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17386 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19656 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17450 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19739 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17364 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19697 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17349 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19709 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17318 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19750 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17315 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19700 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17286 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19816 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17237 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19726 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 57, with loss  0.19496 compared to final loss  0.19726\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_75\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65723 (mse_score:  0.657)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64822 (mse_score:  0.648)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39993 (mse_score:  0.400)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38287 (mse_score:  0.383)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.25657 (mse_score:  0.257)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24687 (mse_score:  0.247)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21433 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20916 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19817 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19590 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19011 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18936 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18693 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18647 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18410 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18418 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18349 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18094 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18374 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18111 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18236 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18048 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18235 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17896 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18198 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17957 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18343 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17899 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18266 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17829 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18266 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17883 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18283 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17865 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18239 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17804 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18219 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17830 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18214 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.18185 compared to final loss  0.18214\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_76\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.53380 (mse_score:  0.534)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.59594 (mse_score:  0.596)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.31341 (mse_score:  0.313)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34960 (mse_score:  0.350)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.21813 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23998 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19508 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20680 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18991 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19494 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18665 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19087 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18542 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18735 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18423 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18539 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18301 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18372 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18268 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18275 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18235 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18167 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18182 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18096 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18119 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18126 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18093 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18095 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18137 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18037 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18205 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18061 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18107 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17928 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18103 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17909 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18051 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17921 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18019 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17893 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 94, with loss  0.17878 compared to final loss  0.17893\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_77\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.61295 (mse_score:  0.613)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55322 (mse_score:  0.553)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.34362 (mse_score:  0.344)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.32374 (mse_score:  0.324)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.21518 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20969 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.19006 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18121 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18571 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17701 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18523 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17540 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18394 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17462 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18234 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17491 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18251 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17490 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18200 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17530 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18139 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17513 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18191 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17454 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18213 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17435 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18193 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17394 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18265 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17439 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18166 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17409 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18221 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17440 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18194 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17429 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18178 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17412 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18161 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17470 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 64, with loss  0.17353 compared to final loss  0.17470\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_78\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.60537 (mse_score:  0.605)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.63722 (mse_score:  0.637)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.41410 (mse_score:  0.414)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42901 (mse_score:  0.429)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.30069 (mse_score:  0.301)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31311 (mse_score:  0.313)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.24259 (mse_score:  0.243)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25522 (mse_score:  0.255)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21424 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22899 (mse_score:  0.229)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19956 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21484 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19132 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20713 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18783 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20276 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18378 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20051 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18154 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19963 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17951 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19833 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17918 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19701 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17842 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19506 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17750 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19577 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17726 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19536 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17740 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19582 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17800 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19489 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17699 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19579 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17762 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19518 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17666 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19425 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.19419 compared to final loss  0.19425\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_79\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.50303 (mse_score:  0.503)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.47869 (mse_score:  0.479)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.29554 (mse_score:  0.296)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.30225 (mse_score:  0.302)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.20130 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22360 (mse_score:  0.224)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.18162 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20627 (mse_score:  0.206)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.17794 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20302 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.17640 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20203 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17654 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20123 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17674 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20095 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17439 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19993 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17543 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19980 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17489 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19921 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17412 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19838 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17480 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19901 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17451 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19816 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17381 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19922 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17427 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19872 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17326 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19817 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17340 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19829 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17354 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19822 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17307 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19739 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.19725 compared to final loss  0.19739\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_80\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.47965 (mse_score:  0.480)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.44128 (mse_score:  0.441)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.27341 (mse_score:  0.273)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23678 (mse_score:  0.237)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.19677 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17459 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.18894 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16953 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18537 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16925 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18489 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16997 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18436 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16993 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18373 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17065 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18269 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17049 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18284 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17119 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18205 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17146 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18223 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17131 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18061 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17180 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18106 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17235 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18140 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17299 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18029 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17311 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18027 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17259 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18034 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17326 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17985 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17361 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18062 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17347 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 25, with loss  0.16925 compared to final loss  0.17347\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_81\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.77895 (mse_score:  0.779)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.78988 (mse_score:  0.790)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.55111 (mse_score:  0.551)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54982 (mse_score:  0.550)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.37150 (mse_score:  0.372)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.36704 (mse_score:  0.367)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25494 (mse_score:  0.255)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25515 (mse_score:  0.255)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20092 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20305 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18465 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18849 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18136 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18696 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18032 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18744 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18033 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18722 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17916 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18649 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17829 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18756 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17791 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18720 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17729 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18740 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17748 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18737 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17710 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18719 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17730 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18759 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17771 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18690 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17618 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18675 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17598 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18750 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17784 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18773 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 39, with loss  0.18633 compared to final loss  0.18773\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_82\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.64162 (mse_score:  0.642)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62289 (mse_score:  0.623)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.38293 (mse_score:  0.383)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38520 (mse_score:  0.385)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.23066 (mse_score:  0.231)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24561 (mse_score:  0.246)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20056 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21892 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19130 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21464 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18669 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21278 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18313 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21164 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18182 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21179 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18121 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21120 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17914 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21102 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17917 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21029 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18050 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21002 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17785 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20940 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17825 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20991 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17858 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20917 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17750 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21017 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17800 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20938 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17696 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20960 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17789 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20815 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17608 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20899 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 98, with loss  0.20807 compared to final loss  0.20899\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_83\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75375 (mse_score:  0.754)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.74628 (mse_score:  0.746)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.53563 (mse_score:  0.536)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.51960 (mse_score:  0.520)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.35353 (mse_score:  0.354)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.34025 (mse_score:  0.340)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25155 (mse_score:  0.252)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24410 (mse_score:  0.244)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21152 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20753 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19562 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19272 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18940 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18756 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18697 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18597 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18482 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18396 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18449 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18277 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18269 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18290 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18315 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18087 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18151 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18057 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18099 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18037 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18147 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17991 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18129 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17996 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18012 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18008 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18022 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17971 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17989 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17954 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17953 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17944 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 91, with loss  0.17889 compared to final loss  0.17944\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_84\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.72603 (mse_score:  0.726)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.86447 (mse_score:  0.864)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.56011 (mse_score:  0.560)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64944 (mse_score:  0.649)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.38333 (mse_score:  0.383)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.42169 (mse_score:  0.422)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.26118 (mse_score:  0.261)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26329 (mse_score:  0.263)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.20983 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19347 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19513 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17119 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19027 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16399 (mse_score:  0.164)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18848 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16275 (mse_score:  0.163)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18699 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16180 (mse_score:  0.162)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18733 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16080 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18562 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16053 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18659 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16054 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18646 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16069 (mse_score:  0.161)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18642 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15951 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18669 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16023 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18594 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15949 (mse_score:  0.159)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18596 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16018 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18632 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16003 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18609 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15992 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18657 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.15977 (mse_score:  0.160)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.15891 compared to final loss  0.15977\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_85\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.58775 (mse_score:  0.588)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.60620 (mse_score:  0.606)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.39115 (mse_score:  0.391)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.40164 (mse_score:  0.402)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.28062 (mse_score:  0.281)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28992 (mse_score:  0.290)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23132 (mse_score:  0.231)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23931 (mse_score:  0.239)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21084 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21540 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19948 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20351 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19187 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19577 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18858 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19237 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18569 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18908 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18507 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18805 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18381 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18610 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18306 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18573 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18373 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18543 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18493 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18215 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18444 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18264 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18431 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18255 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18468 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18163 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18434 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18164 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18342 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18150 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.18333 compared to final loss  0.18387\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_86\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.69725 (mse_score:  0.697)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.62035 (mse_score:  0.620)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.47251 (mse_score:  0.473)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41553 (mse_score:  0.416)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.31245 (mse_score:  0.312)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.28266 (mse_score:  0.283)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23030 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21627 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19901 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19422 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18770 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18796 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18454 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18613 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18219 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18531 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18225 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18531 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18182 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18584 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17999 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18551 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18057 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18619 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18031 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18547 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18012 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18513 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18075 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18484 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17944 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18482 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18005 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18512 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17973 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18510 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17957 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18562 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17949 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18482 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 84, with loss  0.18412 compared to final loss  0.18482\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_87\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.76663 (mse_score:  0.767)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.79063 (mse_score:  0.791)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.50810 (mse_score:  0.508)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.51995 (mse_score:  0.520)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.31626 (mse_score:  0.316)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.32779 (mse_score:  0.328)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.22819 (mse_score:  0.228)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24458 (mse_score:  0.245)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19668 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21554 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18836 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20451 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18522 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19968 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18365 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19800 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18051 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19627 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18070 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19461 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18029 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19293 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17985 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19245 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17862 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19212 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17941 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19263 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17908 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19132 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17894 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19137 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17825 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19111 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17763 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19066 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17869 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19073 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17827 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19102 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 99, with loss  0.18986 compared to final loss  0.19102\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_88\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75974 (mse_score:  0.760)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.84610 (mse_score:  0.846)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.58601 (mse_score:  0.586)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.65177 (mse_score:  0.652)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.40215 (mse_score:  0.402)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.45178 (mse_score:  0.452)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.27424 (mse_score:  0.274)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31662 (mse_score:  0.317)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21842 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25680 (mse_score:  0.257)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19488 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23050 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18445 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21833 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17919 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21195 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17649 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20746 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17441 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20549 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17388 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20468 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17452 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20331 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17340 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20220 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17346 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20156 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17305 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20184 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17260 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20187 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17296 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20020 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17272 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20040 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17196 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20114 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17291 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20057 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 96, with loss  0.20000 compared to final loss  0.20057\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_89\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.84444 (mse_score:  0.844)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.79066 (mse_score:  0.791)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.69262 (mse_score:  0.693)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.64807 (mse_score:  0.648)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.52590 (mse_score:  0.526)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.49362 (mse_score:  0.494)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.37698 (mse_score:  0.377)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35245 (mse_score:  0.352)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.27503 (mse_score:  0.275)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25900 (mse_score:  0.259)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.22168 (mse_score:  0.222)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21166 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19759 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19041 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18923 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18276 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18476 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18071 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18252 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17827 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18116 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17829 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18034 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17771 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18083 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17770 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18017 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17730 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18038 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17717 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18038 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17659 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18089 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17719 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18019 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17639 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18088 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17741 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18015 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17701 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 81, with loss  0.17612 compared to final loss  0.17701\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_90\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.75499 (mse_score:  0.755)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.71900 (mse_score:  0.719)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.61859 (mse_score:  0.619)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.58374 (mse_score:  0.584)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.48568 (mse_score:  0.486)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.46548 (mse_score:  0.465)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.39752 (mse_score:  0.398)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.38982 (mse_score:  0.390)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.34219 (mse_score:  0.342)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.33894 (mse_score:  0.339)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.30040 (mse_score:  0.300)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29916 (mse_score:  0.299)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.27083 (mse_score:  0.271)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27055 (mse_score:  0.271)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.24744 (mse_score:  0.247)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.24840 (mse_score:  0.248)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.23180 (mse_score:  0.232)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23265 (mse_score:  0.233)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.21763 (mse_score:  0.218)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22131 (mse_score:  0.221)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.20994 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21311 (mse_score:  0.213)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.20312 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20877 (mse_score:  0.209)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.19888 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20490 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.19550 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20186 (mse_score:  0.202)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.19245 (mse_score:  0.192)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19882 (mse_score:  0.199)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.19113 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19823 (mse_score:  0.198)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18940 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19666 (mse_score:  0.197)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18889 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19636 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18778 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19523 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18662 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19500 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.19427 compared to final loss  0.19500\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_91\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.73515 (mse_score:  0.735)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.84447 (mse_score:  0.844)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.50927 (mse_score:  0.509)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55731 (mse_score:  0.557)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29720 (mse_score:  0.297)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.31606 (mse_score:  0.316)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20401 (mse_score:  0.204)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21412 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18184 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19072 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.17864 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18940 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17777 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18933 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17718 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18986 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17795 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18980 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17733 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19001 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17650 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18906 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17717 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18990 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17757 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18974 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17660 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18918 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17683 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18953 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17607 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19049 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17638 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19005 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17706 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18942 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17605 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18921 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17585 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18985 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 29, with loss  0.18880 compared to final loss  0.18985\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_92\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.72308 (mse_score:  0.723)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.66830 (mse_score:  0.668)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.48551 (mse_score:  0.486)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.45066 (mse_score:  0.451)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29973 (mse_score:  0.300)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27963 (mse_score:  0.280)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21010 (mse_score:  0.210)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20088 (mse_score:  0.201)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18576 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18137 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18246 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17864 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18098 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17738 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18064 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17774 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18128 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17709 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18055 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17723 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17986 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17686 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18109 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17732 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18009 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17765 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18120 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17700 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18021 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17708 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18106 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17778 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17955 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17669 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17976 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17704 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18058 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17678 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18043 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17728 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 93, with loss  0.17656 compared to final loss  0.17728\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_93\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.56771 (mse_score:  0.568)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.55988 (mse_score:  0.560)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.36951 (mse_score:  0.370)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35636 (mse_score:  0.356)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.24218 (mse_score:  0.242)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22961 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.20323 (mse_score:  0.203)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19403 (mse_score:  0.194)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19531 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18819 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19093 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18553 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18975 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18529 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18693 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18527 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18706 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18412 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18550 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18417 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18508 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18331 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18463 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18417 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18480 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18288 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18298 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18318 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18313 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18219 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18413 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18286 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18351 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18321 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18280 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18218 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18222 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18244 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18240 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18155 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_94\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.69354 (mse_score:  0.694)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.77673 (mse_score:  0.777)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.46966 (mse_score:  0.470)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54173 (mse_score:  0.542)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.29260 (mse_score:  0.293)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35756 (mse_score:  0.358)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.21115 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26286 (mse_score:  0.263)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.18353 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23036 (mse_score:  0.230)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.17636 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.22032 (mse_score:  0.220)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.17413 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21677 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17280 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21683 (mse_score:  0.217)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17235 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21554 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17126 (mse_score:  0.171)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21495 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17049 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21576 (mse_score:  0.216)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17001 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21510 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17008 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21396 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.16977 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21503 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.16975 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21419 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.16988 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21424 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.16947 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21522 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.16910 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21432 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.16964 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21437 (mse_score:  0.214)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.16995 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21482 (mse_score:  0.215)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 88, with loss  0.21374 compared to final loss  0.21482\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_95\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.70012 (mse_score:  0.700)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.71306 (mse_score:  0.713)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.51514 (mse_score:  0.515)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.52292 (mse_score:  0.523)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.35206 (mse_score:  0.352)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.35167 (mse_score:  0.352)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25290 (mse_score:  0.253)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.25091 (mse_score:  0.251)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21129 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20725 (mse_score:  0.207)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.19628 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19114 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18999 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18389 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18640 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17910 (mse_score:  0.179)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18473 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17737 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18397 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17599 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18385 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17521 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18262 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17441 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18332 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17415 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18185 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17390 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18145 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17300 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18191 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17280 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18157 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17227 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18160 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17265 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18039 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17213 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18163 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17286 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 95, with loss  0.17213 compared to final loss  0.17286\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_96\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.65844 (mse_score:  0.658)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.75036 (mse_score:  0.750)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.48594 (mse_score:  0.486)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54900 (mse_score:  0.549)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.36012 (mse_score:  0.360)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.39658 (mse_score:  0.397)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.27785 (mse_score:  0.278)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.29680 (mse_score:  0.297)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.23380 (mse_score:  0.234)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.23998 (mse_score:  0.240)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.21154 (mse_score:  0.212)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.21074 (mse_score:  0.211)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19966 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19493 (mse_score:  0.195)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.19313 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18565 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18968 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18048 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18765 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17789 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18554 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17657 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18387 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17519 (mse_score:  0.175)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18291 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17410 (mse_score:  0.174)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18365 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17296 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18347 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17278 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18226 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17253 (mse_score:  0.173)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18199 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17188 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18295 (mse_score:  0.183)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17204 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18218 (mse_score:  0.182)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17184 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18145 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17185 (mse_score:  0.172)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 97, with loss  0.17150 compared to final loss  0.17185\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_97\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.73217 (mse_score:  0.732)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.85559 (mse_score:  0.856)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.54015 (mse_score:  0.540)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.63990 (mse_score:  0.640)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.36002 (mse_score:  0.360)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.41944 (mse_score:  0.419)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.23799 (mse_score:  0.238)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.27116 (mse_score:  0.271)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.19319 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20847 (mse_score:  0.208)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.18359 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19610 (mse_score:  0.196)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.18098 (mse_score:  0.181)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19283 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.17952 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19141 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.17952 (mse_score:  0.180)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.19067 (mse_score:  0.191)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.17848 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18976 (mse_score:  0.190)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.17731 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18884 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.17848 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18903 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.17780 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18834 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.17657 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18788 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.17671 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18796 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.17640 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18881 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.17741 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18895 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.17569 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18731 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.17660 (mse_score:  0.177)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18787 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.17592 (mse_score:  0.176)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.18873 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 90, with loss  0.18731 compared to final loss  0.18873\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_98\n",
      "13:32 madminer.ml.score    INFO    Starting training\n",
      "13:32 madminer.ml.score    INFO      Batch size:             128\n",
      "13:32 madminer.ml.score    INFO      Optimizer:              amsgrad\n",
      "13:32 madminer.ml.score    INFO      Epochs:                 100\n",
      "13:32 madminer.ml.score    INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "13:32 madminer.ml.score    INFO      Validation split:       0.25\n",
      "13:32 madminer.ml.score    INFO      Early stopping:         True\n",
      "13:32 madminer.ml.score    INFO      Scale inputs:           True\n",
      "13:32 madminer.ml.score    INFO      Shuffle labels          False\n",
      "13:32 madminer.ml.score    INFO      Samples:                all\n",
      "13:32 madminer.ml.score    INFO    Loading training data\n",
      "13:32 madminer.utils.vario INFO      Loading data2/x_train.npy into RAM\n",
      "13:32 madminer.utils.vario INFO      Loading data2/t_xz_train.npy into RAM\n",
      "13:32 madminer.ml.score    INFO    Found 1000 samples with 2 parameters and 1 observables\n",
      "13:32 madminer.ml.base     INFO    Setting up input rescaling\n",
      "13:32 madminer.ml.score    INFO    Creating model\n",
      "13:32 madminer.ml.score    INFO    Training model\n",
      "13:32 madminer.utils.ml.tr INFO    Training on CPU with single precision\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch   5: train loss  0.62846 (mse_score:  0.628)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.54904 (mse_score:  0.549)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  10: train loss  0.45576 (mse_score:  0.456)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.37939 (mse_score:  0.379)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  15: train loss  0.33788 (mse_score:  0.338)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.26871 (mse_score:  0.269)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  20: train loss  0.25951 (mse_score:  0.260)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.20537 (mse_score:  0.205)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  25: train loss  0.21904 (mse_score:  0.219)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.17776 (mse_score:  0.178)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  30: train loss  0.20014 (mse_score:  0.200)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16921 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  35: train loss  0.19277 (mse_score:  0.193)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16701 (mse_score:  0.167)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  40: train loss  0.18887 (mse_score:  0.189)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16809 (mse_score:  0.168)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  45: train loss  0.18762 (mse_score:  0.188)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16897 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  50: train loss  0.18733 (mse_score:  0.187)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16954 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  55: train loss  0.18584 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16903 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  60: train loss  0.18617 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16982 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  65: train loss  0.18488 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16974 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  70: train loss  0.18567 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16982 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  75: train loss  0.18598 (mse_score:  0.186)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16900 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  80: train loss  0.18478 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16977 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  85: train loss  0.18476 (mse_score:  0.185)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16938 (mse_score:  0.169)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  90: train loss  0.18434 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16989 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch  95: train loss  0.18427 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16958 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO      Epoch 100: train loss  0.18421 (mse_score:  0.184)\n",
      "13:32 madminer.utils.ml.tr INFO                 val. loss   0.16999 (mse_score:  0.170)\n",
      "13:32 madminer.utils.ml.tr INFO    Early stopping after epoch 35, with loss  0.16701 compared to final loss  0.16999\n",
      "13:32 madminer.utils.ml.tr INFO    Training time spend on:\n",
      "13:32 madminer.utils.ml.tr INFO                      initialize model:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                   ALL:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                            check data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          make dataset:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       make dataloader:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                       setup optimizer:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   initialize training:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                                set lr:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   load training batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        fwd: move data:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   fwd: check for nans:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                    fwd: model.forward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 fwd: calculate losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 training forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   training sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        opt: zero grad:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                         opt: backward:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                   opt: clip grad norm:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                             opt: step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        optimizer step:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 load validation batch:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO               validation forward pass:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                 validation sum losses:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                        early stopping:   0.00h\n",
      "13:32 madminer.utils.ml.tr INFO                          report epoch:   0.00h\n",
      "13:32 madminer.ml.base     INFO    Saving model to models/sally_99\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_0\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_1\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_2\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_3\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_4\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_5\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_6\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_7\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_8\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_9\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_10\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_11\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_12\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_13\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_14\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_15\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_16\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_17\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_18\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_19\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_20\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_21\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_22\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_23\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_24\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_25\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_26\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_27\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_28\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_29\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_30\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_31\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_32\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_33\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_34\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_35\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_36\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_37\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_38\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_39\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_40\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_41\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_42\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_43\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_44\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_45\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_46\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_47\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_48\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_49\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_50\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_51\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_52\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_53\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_54\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_55\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_56\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_57\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_58\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_59\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_60\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_61\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_62\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_63\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_64\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_65\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_66\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_67\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_68\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_69\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_70\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_71\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_72\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_73\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_74\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_75\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_76\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_77\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_78\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_79\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_80\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_81\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_82\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_83\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_84\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_85\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_86\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_87\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_88\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_89\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_90\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_91\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_92\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_93\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_94\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_95\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_96\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_97\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_98\n",
      "13:32 madminer.ml.base     INFO    Saving model to models2/sally_ensemble/estimator_99\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100\n",
    "estimators = [\n",
    "    ScoreEstimator(n_hidden=(20, 20), activation='relu') for i in range(n_estimators)\n",
    "]\n",
    "ensemble = Ensemble(estimators)\n",
    "for i, _ in enumerate(estimators):\n",
    "    ensemble.train_one(i,\n",
    "        method=\"sally\",\n",
    "        x=\"data2/x_train.npy\",\n",
    "        t_xz=\"data2/t_xz_train.npy\",\n",
    "        n_epochs=100,\n",
    "    )\n",
    "    estimators[i].save(f\"models/sally_{i}\")\n",
    "\n",
    "ensemble.save(f\"models2/sally_ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_param_points_test = 10000  # number of parameter points to test\n",
    "theta_test = np.zeros(shape=(2,n_param_points))\n",
    "x_test, _ = simulate(theta_test)\n",
    "x_test = np.sort(x_test, axis=0)\n",
    "\n",
    "np.save(\"data2/x_test.npy\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:31 madminer.ml.base     INFO    Loading model from models2/sally\n",
      "10:31 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:31 madminer.ml.base     INFO    Loading evaluation data\n",
      "10:31 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "10:31 madminer.ml.base     INFO    Calculating Fisher information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.83779911, 0.83656491],\n",
       "       [0.83656491, 0.83545665]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sally = ScoreEstimator()\n",
    "sally.load(\"models2/sally\")\n",
    "\n",
    "t_hat_sally = sally.evaluate(\n",
    "    theta=np.array([0,0]),\n",
    "    x=\"data2/x_test.npy\",\n",
    ")\n",
    "sally.calculate_fisher_information(x=\"data2/x_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:31 madminer.ml.base     INFO    Loading model from models2/repulsive_ensemble_sally\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:31 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "10:31 madminer.ml.score    INFO    Loading evaluation data\n",
      "10:31 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "10:31 madminer.ml.score    INFO    Calculating Fisher information\n"
     ]
    }
   ],
   "source": [
    "repulsive_ensemble_sally = RepulsiveEnsembleScoreEstimator()\n",
    "repulsive_ensemble_sally.load(\"models2/repulsive_ensemble_sally\")\n",
    "\n",
    "t_hat_repulsive_ensemble_sally, t_hat_sig_repulsive_ensemble_sally  = repulsive_ensemble_sally.evaluate(\n",
    "    theta=np.array([0]),\n",
    "    x=\"data2/x_test.npy\",\n",
    ")\n",
    "info_repulsive_ensemble, info_cov_repulsive_ensemble = repulsive_ensemble_sally.calculate_fisher_information(x=\"data2/x_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:48 madminer.ml.ensemble INFO    Found score ensemble with 100 estimators\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_0\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_1\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_2\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_3\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_4\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_5\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_6\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_7\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_8\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_9\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_10\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_11\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_13\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_14\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_15\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_16\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_17\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_18\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_19\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_20\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_21\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_22\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_23\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_24\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_25\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_26\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_27\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_28\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_29\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_30\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_31\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_32\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_33\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_34\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_35\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_36\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_37\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_38\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_39\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_40\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_41\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_42\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_43\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_44\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_45\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_46\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_47\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_48\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_49\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_50\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_51\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_52\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_53\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_54\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_55\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_56\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_57\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_58\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_59\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_60\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_61\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_62\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_63\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_64\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_65\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_66\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_67\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_68\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_69\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_70\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_71\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_72\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_73\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_74\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_75\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_76\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_77\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_78\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_79\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_80\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_81\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_82\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_83\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_84\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_85\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_86\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_87\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_88\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_89\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_90\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_91\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_92\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_93\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_94\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_95\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_96\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_97\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_98\n",
      "08:48 madminer.ml.base     INFO    Loading model from models2/sally_ensemble/estimator_99\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 1 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 2 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 3 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 4 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 5 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 6 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 7 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 8 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 9 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 10 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 11 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 12 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 13 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 14 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 15 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 16 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 17 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 18 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 19 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 20 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 21 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 22 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 23 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 24 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 25 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 26 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 27 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 28 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 29 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 30 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 31 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 32 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 33 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 34 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 35 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 36 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 37 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 38 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 39 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 40 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 41 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 42 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 43 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 44 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 45 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 46 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 47 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 48 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 49 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 50 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 51 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 52 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 53 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 54 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 55 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 56 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 57 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 58 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 59 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 60 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 61 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 62 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 63 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 64 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 65 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 66 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 67 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 68 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 69 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 70 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 71 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 72 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 73 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 74 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 75 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 76 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 77 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 78 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 79 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 80 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 81 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 82 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 83 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 84 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 85 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 86 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 87 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 88 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 89 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 90 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 91 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 92 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 93 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 94 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 95 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 96 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 97 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 98 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 99 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.ml.ensemble INFO    Starting evaluation for estimator 100 / 100 in ensemble\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n",
      "08:48 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n"
     ]
    }
   ],
   "source": [
    "sally_ensemble = Ensemble()\n",
    "sally_ensemble.load(\"models2/sally_ensemble\")\n",
    "\n",
    "t_hat_sally_ensemble, _, t_hat_sig_sally_ensemble = sally_ensemble.evaluate_score(\n",
    "    # theta=np.array([0]),\n",
    "    theta=0,\n",
    "    x=\"data2/x_test.npy\",\n",
    "    calculate_covariance=False,\n",
    "    calculate_sigma=True\n",
    ")\n",
    "info_sally_ensemble, info_cov_sally_ensemble = sally_ensemble.calculate_fisher_information(x=\"data2/x_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:52 madminer.ml.score    INFO    Loading evaluation data\n",
      "08:52 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:52 madminer.ml.score    INFO    Calculating Fisher information\n",
      "08:52 madminer.utils.vario INFO      Loading data2/x_test.npy into RAM\n"
     ]
    }
   ],
   "source": [
    "infos_repulsive_ensemble, infos_cov_repulsive_ensemble = repulsive_ensemble_sally.calculate_fisher_information(x=\"data2/x_test.npy\", sum_events=False)\n",
    "infos_sally_ensemble, infos_cov_sally_ensemble = sally_ensemble.calculate_fisher_information(x=\"data2/x_test.npy\", sum_events=False)\n",
    "infos_true = calculate_true_info(x_test, [0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figs, axs = plt.subplots(2,2)\n",
    "\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i,j].plot(x_test, infos_repulsive_ensemble[:,i,j], label=f\"I{i}{j} (repulsive ensemble)\", ls='-', c='b')\n",
    "        axs[i,j].fill_between(x_test, infos_repulsive_ensemble[:,i,j] + np.sqrt(np.einsum(\"nijkl->nil\", infos_cov_repulsive_ensemble)[:,i,j]), infos_repulsive_ensemble[:,i,j] - np.sqrt(np.einsum(\"nijkl->nil\", infos_cov_repulsive_ensemble)[:,i,j]), color='b', alpha=0.2)\n",
    "        axs[i,j].plot(x_test, infos_sally_ensemble[:,i,j], label=f\"I{i}{j} (normal ensemble)\", ls='-', c='g')\n",
    "        axs[i,j].plot(x_test, infos_true[:,i,j], label=f\"I{i}{j} (true)\", ls='-', c='r')\n",
    "        axs[i,j].legend(ncol=2, fontsize=6)\n",
    "\n",
    "        axs[i,j].set_xlim([-1,1])\n",
    "        axs[i,j].set_ylim([-2e-5,1e-4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(\n",
    "    fisher_information_matrices,\n",
    "    fisher_information_covariances=None,\n",
    "    reference_thetas=None,\n",
    "    contour_distance=2.4476,\n",
    "    xlabel=r\"$\\theta_0$\",\n",
    "    ylabel=r\"$\\theta_1$\",\n",
    "    xrange=(-1.0, 1.0),\n",
    "    yrange=(-1.0, 1.0),\n",
    "    labels=None,\n",
    "    inline_labels=None,\n",
    "    resolution=500,\n",
    "    colors=None,\n",
    "    linestyles=None,\n",
    "    linewidths=1.5,\n",
    "    alphas=1.0,\n",
    "    alphas_uncertainties=0.25,\n",
    "    scale_x = 1.,\n",
    "    scale_y = 1.,\n",
    "    HARRY = None,\n",
    "):\n",
    "        \n",
    "    # Input data\n",
    "    fisher_information_matrices = np.asarray(fisher_information_matrices)\n",
    "\n",
    "    n_matrices = fisher_information_matrices.shape[0]\n",
    "    \n",
    "    if HARRY is None:\n",
    "        HARRY = n_matrices * [False]\n",
    "\n",
    "    if fisher_information_matrices.shape != (n_matrices, 2, 2):\n",
    "        raise RuntimeError(\n",
    "            \"Fisher information matrices have shape {}, not (n, 2,2)!\".format(fisher_information_matrices.shape)\n",
    "        )\n",
    "\n",
    "    if fisher_information_covariances is None:\n",
    "        fisher_information_covariances = [None for _ in range(n_matrices)]\n",
    "\n",
    "    if reference_thetas is None:\n",
    "        reference_thetas = [None for _ in range(n_matrices)]\n",
    "\n",
    "    d2_threshold = contour_distance ** 2.0\n",
    "\n",
    "    # Line formatting\n",
    "    if colors is None:\n",
    "        colors = [\"C\" + str(i) for i in range(10)] * (n_matrices // 10 + 1)\n",
    "    elif not isinstance(colors, list):\n",
    "        colors = [colors for _ in range(n_matrices)]\n",
    "\n",
    "    if linestyles is None:\n",
    "        linestyles = [\"solid\", \"dashed\", \"dotted\", \"dashdot\"] * (n_matrices // 4 + 1)\n",
    "    elif not isinstance(linestyles, list):\n",
    "        linestyles = [linestyles for _ in range(n_matrices)]\n",
    "\n",
    "    if not isinstance(linewidths, list):\n",
    "        linewidths = [linewidths for _ in range(n_matrices)]\n",
    "\n",
    "    if not isinstance(alphas, list):\n",
    "        alphas = [alphas for _ in range(n_matrices)]\n",
    "\n",
    "    if not isinstance(alphas_uncertainties, list):\n",
    "        alphas_uncertainties = [alphas_uncertainties for _ in range(n_matrices)]\n",
    "\n",
    "    # Grid\n",
    "    xi, yi, xx, yy, thetas = n_matrices * [None], n_matrices * [None], n_matrices * [None], n_matrices * [None], n_matrices * [None]\n",
    "    print(len(HARRY), len(xi))\n",
    "    for i, harry in enumerate(HARRY):\n",
    "        if harry:\n",
    "            xi[i] = np.linspace(np.sign(xrange[0])*xrange[0]**2, \n",
    "                                np.sign(xrange[1])*xrange[1]**2, resolution)\n",
    "            yi[i] = np.linspace(np.sign(yrange[0])*yrange[0]**2, \n",
    "                                np.sign(yrange[1])*yrange[1]**2, resolution)\n",
    "        else:\n",
    "            xi[i] = np.linspace(xrange[0], xrange[1], resolution)\n",
    "            yi[i] = np.linspace(yrange[0], yrange[1], resolution)\n",
    "        xx[i], yy[i] = np.meshgrid(xi[i], yi[i], indexing=\"xy\")\n",
    "        xx[i], yy[i] = xx[i].flatten(), yy[i].flatten()\n",
    "        thetas[i] = np.vstack((xx[i], yy[i])).T\n",
    "\n",
    "    # Theta from reference thetas\n",
    "    d_thetas = []\n",
    "    for reference_theta, theta in zip(reference_thetas, thetas):\n",
    "        if reference_theta is None:\n",
    "            d_thetas.append(theta)\n",
    "        else:\n",
    "            d_thetas.append(theta - reference_theta)\n",
    "    d_thetas = np.array(d_thetas)  # Shape (n_matrices, n_thetas, n_parameters)\n",
    "\n",
    "    # Calculate Fisher distances\n",
    "    fisher_distances_squared = np.einsum(\"mni,mij,mnj->mn\", d_thetas, fisher_information_matrices, d_thetas)\n",
    "    fisher_distances_squared = fisher_distances_squared.reshape((n_matrices, resolution, resolution))\n",
    "\n",
    "    # Calculate uncertainties of Fisher distances\n",
    "    fisher_distances_squared_uncertainties = []\n",
    "    for d_theta, inf_cov in zip(d_thetas, fisher_information_covariances):\n",
    "        if inf_cov is None:\n",
    "            fisher_distances_squared_uncertainties.append(None)\n",
    "            continue\n",
    "\n",
    "        var = np.einsum(\"ni,nj,ijkl,nk,nl->n\", d_theta, d_theta, inf_cov, d_theta, d_theta)\n",
    "\n",
    "        uncertainties = (var ** 0.5).reshape((resolution, resolution))\n",
    "        fisher_distances_squared_uncertainties.append(uncertainties)\n",
    "\n",
    "#        logger.debug(\"Std: %s\", uncertainties)\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots()\n",
    "#    fig = plt.figure(figsize=(5.0, 5.0))\n",
    "\n",
    "    # Error bands\n",
    "    for i in range(n_matrices):\n",
    "        if fisher_information_covariances[i] is not None:\n",
    "            d2_up = fisher_distances_squared[i] + fisher_distances_squared_uncertainties[i]\n",
    "            d2_down = fisher_distances_squared[i] - fisher_distances_squared_uncertainties[i]\n",
    "            band = (d2_up > d2_threshold) * (d2_down < d2_threshold) + (d2_up < d2_threshold) * (d2_down > d2_threshold)\n",
    "\n",
    "            if HARRY[i]:\n",
    "                xi_plot = np.sqrt(np.abs(xi[i])) * np.sign(xi[i])\n",
    "                yi_plot = np.sqrt(np.abs(yi[i])) * np.sign(yi[i])\n",
    "            else:\n",
    "                xi_plot = xi[i]\n",
    "                yi_plot = yi[i]\n",
    "            plt.contourf(xi_plot, yi_plot, band, [0.5, 2.5], colors=colors[i], alpha=alphas_uncertainties[i])\n",
    "\n",
    "    # Predictions\n",
    "    for i in range(n_matrices):\n",
    "        if HARRY[i]:\n",
    "            xi_plot = np.sqrt(np.abs(xi[i])) * np.sign(xi[i])\n",
    "            yi_plot = np.sqrt(np.abs(yi[i])) * np.sign(yi[i])\n",
    "        else:\n",
    "            xi_plot = xi[i]\n",
    "            yi_plot = yi[i]\n",
    "        cs = ax.contour(\n",
    "            xi_plot,\n",
    "            yi_plot,\n",
    "            fisher_distances_squared[i],\n",
    "            np.array([d2_threshold]),\n",
    "            colors=colors[i],\n",
    "            linestyles=linestyles[i],\n",
    "            linewidths=linewidths[i],\n",
    "            alpha=alphas[i],\n",
    "            # label=None if labels is None else labels[i],\n",
    "        )\n",
    "\n",
    "        if inline_labels is not None and inline_labels[i] is not None and len(inline_labels[i]) > 0:\n",
    "            plt.clabel(cs, cs.levels, inline=True, fontsize=12, fmt={d2_threshold: inline_labels[i]})\n",
    "\n",
    "    # Legend and decorations\n",
    "    if labels is not None:\n",
    "        ax.legend()\n",
    "\n",
    "    # Scale the x and y axes by some factor\n",
    "    ticks_x = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/scale_x))\n",
    "    ax.xaxis.set_major_formatter(ticks_x)\n",
    "    ticks_y = ticker.FuncFormatter(lambda x, pos: '{0:g}'.format(x/scale_y))\n",
    "    ax.yaxis.set_major_formatter(ticks_y)\n",
    "    \n",
    "    # Set Limits\n",
    "    ax.set_xlim(xrange)\n",
    "    ax.set_ylim(yrange)\n",
    "    \n",
    "    # Set Labels\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    #ax.tight_layout()\n",
    "    fig.subplots_adjust(left=0.18,bottom=0.16,top=0.94,right=0.96, hspace=0.0, wspace=0.0)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plot_contours(\n",
    "    [info_repulsive_ensemble, info_sally_ensemble, infos_true.sum(axis=0)],\n",
    "    [info_cov_repulsive_ensemble, info_cov_sally_ensemble, None],\n",
    "    xrange=(1, 2.0), \n",
    "    yrange=(1, 2.0)\n",
    ")\n",
    "legend_elements = [ \n",
    "        Line2D([0],[0], color='b',      lw=2, ls='solid',  label=r'repulsive ensemble'),\n",
    "        Line2D([0],[0], color='orange', lw=2, ls='dashed', label=r'normal ensemble'),\n",
    "        Line2D([0],[0], color='green',  lw=2, ls='dotted', label=r'truth'),\n",
    "    ]\n",
    "plt.legend(handles=legend_elements, frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
